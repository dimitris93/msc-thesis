\chapter{Introduction}

\section{Stochastic Simulation for IR Research}\label{1.1}

In the field of Information Retrieval (IR), the reliable evaluation of systems is a key component in order to progress the state-of-the-art. One method of evaluation, is through the use of test-collections, which are composed of three main components: a set of documents, a set of topics and a set of relevance judgments (the ground truth). The Text REtrieval Conference (TREC\footnote{\url{https://trec.nist.gov/overview.html}}) provides such collections to the IR research community, as well as evaluation software which allows researchers to evaluate their (text retrieval) systems, in an offline setting. The evaluation results of a system are given in the form of per-topic scores and by then averaging these scores, a single system-score can be obtained.

Much of IR research focuses on optimizing the various aspects of such offline experiments. Stochastic simulation is one technique that can be used to assist this kind of research, and it allows us to overcome some limitations associated with IR data. Firstly, through simulation it is possible to generate \textit{endless} amounts of data. This is generally helpful because IR data tends to be limited in size, for example due to the high costs associated with judging the relevance of documents on topics, which is typically done manually by human assessors. Secondly, simulation relies on statistical models that are fitted on data and describe some underlying population (i.e., the scores of systems on topics, or the clicks of users on hyperlinks). The advantage of drawing samples from statistical models, is that not only do we generate $N$ observations, but now we also know some \textit{true} characteristics about the underlying population from which these observations came from, i.e., the true mean value or the variance. Knowledge of such \textit{true} characteristics would have otherwise been impossible, given the fact that the set of documents and queries can potentially be infinite. Thirdly, if we use models that are flexible enough, it is possible to generate data under some given condition, so that the resulting data have specific, desired characteristics. For example, we might want to study two systems that have a target difference in overall performance. In such cases, simulation can be useful, especially when our data contains a few (or no) observations with the desired characteristics we are looking for.

Simulation in IR is not a new concept. In the early days of IR, prior to the development and availability of large test-collections, simulation was used in an attempt to generate the entire test-collections themselves \cite{Cooper1973, Tague1980}. In more recent years, simulation has been used to model various aspects of human interaction with the IR systems \cite{White2005, White2006, Azzopardi2011, Baskaya2013, Maxwell2016}. The term \textit{human interaction} refers to how the user interacts with the system, for example formulating queries, clicking on results, re-formulating queries and so on. In \cite{Azzopardi2007}, simulation was used to generate queries by selecting a document, the known-item, and producing a query for that known-item. This approach has the advantage that no additional relevance judgments are required, since the relevant document is simply the specified known-item. In \cite{Robertson2012}, document scores are simulated in order to study the inherent noise that the per-topic evaluation scores carry due to the fact that test-collections contain a mere \textit{sample} of documents rather than the entire \textit{population} of documents.

In this thesis, we focus on a specific kind of simulation, which is the simulation of evaluation scores of systems on topics. More specifically, we explore the method proposed by Urbano and Nagler in \cite{Urbano2018}. This particular simulation approach, uses existing collections of system scores, to build a model for the joint distribution of system scores on topics, which can then be used to endlessly simulate scores by the same systems, but on random new topics. The simulation has two separate components: one \textit{marginal} model for each system, which models the individual distribution of scores of the system (regardless of the other systems) and a \textit{copula} that models the dependence among systems, meaning, how they tend to perform on the same topic. The main advantage of a copula approach, over classical multivariate models (i.e., the multivariate Gaussian distribution), is flexibility. For example, the marginal distribution of scores of each system, can be modeled with an entirely different distribution family. This is particularly useful for IR data, since not all systems follow the same distribution. Furthermore, it allows flexibility on the modeling of the dependence as well. For example, in one case the dependence could be stronger for small values of the system scores, and weaker for large values, and in another case it could be the other way around. Or, in different case, the dependence could be highly symmetric. 

One application of this stochastic simulation, is to help researchers answer questions such as \textit{"how many topics do we need to achieve a certain level of confidence in our evaluation results?"} \cite{Carterette2009, Urbano2016}. Previous work has largely relied on data-oriented approaches that repeatedly split the topic set in two halves and treat one of the halves as the ground truth and the other half as the actual test-collection. Then, a statistic of agreement between the evaluation results on each of the two splits is computed, for example Kendall's $\tau$ \cite{Zobel1998, Voorhees2002, Urbano2013a}. By extrapolating these observations, it is possible to obtain empirical estimates regarding the reliability of a test-collection, at a target number of topics. This so called \textit{split-half} approach is limited due to the small amounts of available data, the fact that the ground truth is not \textit{actual} ground truth, and that extrapolation is required. Beyond this approach, there is another one that relies on statistical theory, for example, \textit{test theory} \cite{Bodoff2007}. This kind of work is limited as well, due to the fact that it relies on various assumptions, that are typically not satisfied by IR data.

A second application of this stochastic simulation is to help researchers answer questions such as \textit{"which statistical significance test is optimal for IR evaluation data?"}. Statistical significance tests can be used to determine if a small observed difference in mean system performance has occurred due to chance, or not. This is because random errors can occur due to the fact that systems are evaluated on a mere \textit{sample} of topics, rather than the entire (possibly infinite) \textit{population} of topics. Even though these tests are heavily used in IR related research papers \cite{Sakai2016b, Carterette2017}, it is still quite unclear which test is optimal for IR, and a lot of previous work contradicts each other. Statistical significance tests rely on various assumptions that are not actually satisfied by IR evaluation data. Earlier research argued on theoretical grounds about the robustness of statistical significance tests to having their assumptions violated \cite{Rijsbergen1979, Hull1993}, but did not provide any empirical evidence. In later years, works such as \cite{Zobel1998, Sanderson2005, Urbano2013a}, provided empirical results, by splitting the topic set in half, running a test on each split, and computing the agreement rate of the tests between splits. However, their results are limited by the fact that the tests can be consistently wrong on both splits. In \cite{Smucker2007}, the authors compared various statistical significance tests, by measuring their agreement with the permutation test. This work is also limited since its conclusions are based on the assumption that the permutation test is optimal.

Stochastic simulation is a useful tool that can be used to overcome many of these aforementioned limitations that are found in previous work. With regards to the second example application that we mentioned (about finding the optimal statistical significance test for IR evaluation data), stochastic simulation has been employed in earlier works such as \cite{Wilbur1994, Carterette2012, Carterette2015}, however the models were simplistic and likely unrealistic. More recently, there have been two parallel lines of work that use more sophisticated simulation approaches. On the one hand, Urbano et al. \cite{Urbano2019} use a simplified version of the simulation model proposed in \cite{Urbano2018, Urbano2016}, to generate paired scores for a given pair of systems, on \textit{new random topics}. The reason why they focus on only two systems is because they only study paired statistical significance tests, which is the most common use case. On the other hand, Parapar et al. \cite{Parapar2020, Parapar2021} study the same problem, but instead simulate new random system \textit{runs}\footnote{The run of system \textit{s} on topic \textit{t} refers to the resulting ranked list of documents that is produced when \textit{s} is queried about $t$. However, the authors actually generate \textit{relevance profiles}, not runs. The difference is that instead of providing ranked lists of documents, they only provide ranked lists of (binary) relevance values.} for the \textit{same topics}.

\section{Motivation}

Surprisingly, the two aforementioned parallel lines of work of Urbano et al. and Parapar et al. reach different conclusions, despite the fact that both perform stochastic simulation.

In short, the works of Urbano et al. point in the direction of \textit{t-test} being optimal and advocate for the discontinuation of the \textit{Wilcoxon} test. Whereas the works of Parapar et al. make almost the exact opposite recommendations, advocating in favor of the \textit{Wilcoxon} test. Interestingly, those two tests appear to be the most popular tests \cite{Carterette2017}, with the t-test being used about 65\% of time and the Wilcoxon about 25\%. It is therefore important to investigate the reason why these two studies reach opposite conclusions and determine which of the tests is actually optimal. 

On the one side, the main criticism of Urbano et al. aimed at Parapar et al. is that in order to study the tests "one needs to simulate new topics for the same systems" \cite{Urbano2019}. This is because the significance tests are essentially trying to deal with the errors due to the sampling of topics. Another concern is that the simulation of Parapar et al. simulates retrieval scores\footnote{The term retrieval score refers to the score that the system itself gives to each document, in order to rank the documents from best to worst, during the retrieval process.} (as opposed to directly simulating effectiveness scores), which are then used to compute the effectiveness scores. As a consequence, the error of the simulation of retrieval scores, propagates to the effectiveness scores, to an extent that we do not know. Moreover, in the experiments of Parapar et al., the performance of a retrieval system is shifted across all topics, to obtain a better or worse system. However, in reality, systems may improve on some topics, but may not on others. Lastly, the coverage and scale of the study of Parapar et al. was small, and because we are interested in the behavioral trends of the systems (rather than specific cases), a wide range of factors needs to be studied. 

On the other side, the main criticism of Parapar et al. aimed at Urbano et al. is that the quality of the simulation is unknown. It is argued that the simulated data could be biased towards specific tests, due to the fact that "models are fitted from pre-selected classes of distributions" \cite{Parapar2021}. For example, if the simulated data come from a statistical model whose assumptions align with the assumptions of a certain test, that would favor the said test. So the results could be an artifact of the simulation. Furthermore, it is argued that, "the best fit for each combination of measure and retrieval system might be still a poor fit". In other words, they argue that the statistical models used to perform the simulation, despite their flexibility, may still be not good enough to describe IR evaluation data.

To some extend the concerns of Parapar et al. have been addressed in \cite{Urbano2021}, arguing that a variety of both parametric and non-parametric families were used, including distributions based on Kernel Smoothing, which are as free of assumptions as they can be. Furthermore, it was shown that if some of the marginal distribution or copula families were in fact biased towards some tests; that bias would actually favor the \textit{Wilcoxon} test. However, the authors did not provide any empirical results regarding the quality of the simulation, which is precisely the objective of this thesis.

\section{Research Goals}

In this thesis, we try to shed some light on this disagreement between the conclusions of the lines of work of Urbano et al. and Parapar et al. Our main goal is to provide empirical evidence regarding the quality of the simulation proposed in \cite{Urbano2018}. The idea is that if the simulation is of high quality, then this should further validate the recommendations made in \cite{Urbano2019}, advocating for the use of the \textit{t-test} and discontinuation of the \textit{Wilcoxon} test.

We essentially try to answer the question: \textit{"how good is the stochastic simulation proposed in \cite{Urbano2018}?"}. To this end, we evaluate the statistical models used to perform the simulation, in terms of how well they fit (or describe) the data; we call this: \textit{goodness-of-fit}.

We explore the following:

\begin{itemize}
	\item How well can copula-based models capture the joint distribution of IR system scores on topics?
	
	\item How can we improve the quality of a copula-based simulation approach, for the purposes of stochastically simulating scores of IR systems on new random topics?
	
%	\item How well can various bi-variate copula-based models capture the joint distribution of the scores of IR systems on topics?
%		
%	\item How do various model selection criteria compare against each other, for the purposes of stochastically simulating IR system scores on new random topics, using bi-variate copula-based models?	
\end{itemize}

\section{Main Findings}

%In order to measure \textit{how well} a statistical model describes the data, we utilize the split-half approach. The main findings are as follows:

\begin{itemize}
	\item Overall, both the marginal models and the copulas (to a lesser extent) fit the data moderately well.
	\item All marginal and copula families perform fairly consistently when they are selected (by AIC), with the exception of Beta Kernel Smoothing. 
%	\item The marginal models, provide a better fit in the case of discrete metrics (P@10 and RR) compared to continuous metrics (AP, nDCG@20 and ERR@20), whereas the copulas behave more consistently across all effectiveness metrics.
	\item The high appearance of zero scores in the data, is a special case where none of our marginal candidate families appear to be flexible enough to describe well.
	\item We proposed a new model selection criterion, inspired by the split-half approach, that is able to select copula models more optimally than other criteria such as AIC, BIC and LL. This is consistent across all effectiveness measures.
\end{itemize}

\section{Thesis Outline}

The remainder of this thesis is structured as follows. In Chapter \ref{ch:2}, we provide a description of the particular stochastic simulation approach which we explore in this thesis, as well as how it was used in previous work. In Chapter \ref{ch:3}, we study the marginal models (separately from the copulas). We describe the approach we devised for measuring goodness-of-fit, and then employ the said approach. We present and discuss our results, and proceed to explore outliers. We explore ways of improving the quality of the margins, and propose a new selection criterion. Lastly, we provide some empirical evidence, that addresses a limitation of our methodology. In Chapter \ref{ch:4}, we study the copula models (separately from the margins), by employing approaches analogous with the ones used in the case of the margins. In Chapter \ref{ch:5}, we conclude this thesis by discussing our main findings and their implications, and provide future work directions.




