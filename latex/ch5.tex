\chapter{Conclusion}\label{ch:5}

In this thesis, we made a first attempt at providing empirical evidence regarding the quality of the stochastic simulation used in \cite{Urbano2019}. This particular simulation approach, uses existing collections of system scores, to build a model for the joint distribution of system scores on topics, which can then be used to endlessly simulate scores by the same systems, but on random new topics. The simulation is based on two separate components: one \textit{marginal} model for each system, which models the individual distribution of scores of the system, and a \textit{copula} that models the dependence among systems. This allows us to study the marginal models and the copulas separately. 

Measuring the quality of a stochastic simulation is not a trivial or one dimensional task. We focus on one, but highly important aspect of simulation, which is the goodness-of-fit of the models; meaning, how well do the models describe the data. Ideally, this would be measured by computing some similarity metric between a model $F^*$ and the true distribution $F$. However, having knowledge of the true distribution of a system's scores is not feasible. 

For this reason, we resort to the split-half approach, which works by repeatedly splitting the data in half, and treating the first half as \textit{the sample}, and the second half as \textit{the population}. Using the first half of the data to fit a model ($F_1^*$), and the empirical distribution of the second half ($F_2$) as an estimate of the ground truth, we can compute a measure of distance $\Delta$ between $F_1^*$ and $F_2$. The goodness-of-fit can be defined as the negative of that $\Delta$. In addition, we devised a method for calculating a reference value which we should approximately expect from a good fit, by computing the $\Delta$ between the two empirical distributions of the data. We performed a total of \num{250000} random splits for the margins and the copulas respectively, using a variety of IR data.

For the case of the margins, our results show that the models fit the data moderately well, when they are selected by AIC, with the exception of the Beta Kernel Smoothing distribution, which is an outlier. We explored this outlier further, and discovered that part of the explanation behind this is the high appearance of zero scores in the data. It appears that Beta KS models tend to be too simple and underfitted to capture those types of data, yet they are still selected by AIC. We found that the exclusion of Beta KS from the list of candidates, notably improves the overall goodness-of-fit, but not as much as we would have hoped. In fact, we discovered that none of the considered candidate families would have performed well enough in those particular corner cases, even if they were selected optimally. This implies that in order to further improve the quality of the margins in those specific cases, beyond the exclusion of Beta KS, more candidate models would need to be considered. One of such models, could be a mixture model that models the zero scores separately from non-zeros. We leave this for future work.

In view of the fact that there is room for improvement with regards to the quality of the margins, we shift our focus on refining it. In this work, instead of focusing on expanding the list of candidate models with additional ones, we chose to focus on how to select them in a more optimal manner. Our motivation behind this is the fact that we previously identified some cases where AIC tends to make poor choices, as well as the fact that the list of candidates is quite diverse as is; including a variety of both parametric and non-parametric distributions. We proposed a new selection criterion that is inspired by the split-half approach, which we denote as SHC (Split-Half Criterion), and also considered some other well established model selection criteria beyond AIC, such as BIC and LL. We found that for the case of continuous metrics (especially nDCG@20), SHC selects models considerable better than the rest criteria; although, for the case of discrete metrics (especially RR), it performs considerably worse. However, we found that the best approach for maximizing the overall goodness-of-fit of the margins, is to simply exclude Beta KS from the list of candidates, and select models based on either AIC or BIC. This approach works more consistently across the different effectiveness metrics.

For the case of the copulas, our results show that the models fit the data somewhat worse than the margins, but still within reason, when they are selected by AIC. We experimented with other criteria, such as BIC, LL as well as our proposed criterion. Interesting, we found that our criterion provides the best overall goodness-of-fit, consistently across all effectiveness measures. Although, the improvement over AIC is relatively small. To further improve the performance of the copulas, more candidate copula families would need to be considered. Due to the fact that the copula which is selected with the highest frequency (Tawn copula) happens to be the only asymmetric copula in the list of candidates, we speculate that the inclusion of more asymmetric copulas in the simulation, may improve the quality. We also found some evidence that support this hypothesis. This is something we do not experiment with however, leaving it for future work.

Due to the inherent limitation of the split-half approach, of halving the data; we can only measure the goodness-of-fit of models that are fitted on half the data. However, we are interested on models that are fitted on the entire set of data. For this reason, we also investigated if we underestimate or overestimate goodness-of-fit, and by how much. This was explored using a data collection that contains a larger set of topics, which allowed us to extrapolate our findings to larger topic set sizes. We found that we underestimate the performance of the margins as well as the copulas, but only slightly. Another consequence of halving the data, is that the candidate margin and copula distribution families, are not selected with the same frequency when the models are fitted on half the data, compared to the whole data. We explored this matter as well, and found that the differences are fairly minor. These findings suggest that our goodness-of-fit estimates should be quite accurate. 

One highly important implication of our findings, is that due to the fact that both the marginal models, and the copula models (to a lesser extent), describe the data moderately well, this adds a high degree of reliability in the findings of Urbano et al. in \cite{Urbano2019}. More specifically, the conclusions reached, regarding the t-test and the permutation tests being the most optimal, and the sign, Wilcoxon and bootstrap-shift being the least optimal, for IR evaluation data. 

At the same time, due to the fact that the concerns of Parapar et al. with regards to the quality of the simulation used by Urbano et al. have largely been addressed in this thesis, the question remains as to precisely why are their conclusions not in accordance. This requires further investigation. As suggested in \cite{Urbano2021}, an important direction for future work is to compare the two simulation approaches, and their findings, in a controlled setting. This is important, not only for helping us determine which statistical significance test is optimal in IR and when, but also, for deepening our knowledge with regards to the properties of IR evaluation data.



%\begin{itemize}
%	\item Margins fit the data moderately well. We identified some corner cases where AIC made the wrong selection. This occurrence can partly be explained by the high appearance of zeros. Due to the fact that AIC can be wrong, coupled with the fact that the margins have room for improvement, we experimented with alternative ways to select models, and also proposed a new selection criterion. The overall goodness-of-fit is slightly improved, if we simply remove the outlier (Beta KS) from the list of candidate models, and continue using AIC. To further increase the goodness-of-fit would require additional candidate models to be considered, for example, a mixture models that models zeros separately. Due to the inherent limitation of a split-half approach, we actually measure the goodness-of-fit of models that are fitted on 25 topics, instead of 50, which is not what we want. For this reason, we extrapolated our results, and found that the goodness-of-fit is slightly underestimated.
%	\item Copulas fit the data somewhat worse than what we hoped for, but still within reason. We experimented again with various selection criteria, and found that ours was consistently better, but a small amount. To further improve the goodness-of-fit, will likely require additional candidates. Our hypothesis is that the fit is not as good as expected, due to the lack of asymmetric copulas in the list of candidates. Extrapolating our results to 50 topics, we find that we underestimate the goodness-of-fit by a small amount.
%	\item One implication of these results, is that since the quality of the margins/copulas is moderately good / within reason, consequently the results with regards to "which test is optimal" should also be relatively reliable. 
%	\item Future work: Study higher dimension copulas. Study distribution of per-topic score differences. Add a mixture model to the list of marginal candidates. Add more asymmetric copulas to the list of copula candidates.
%
%\end{itemize}

%\section{Recommendations}
%	\begin{itemize}
%		\item Since the simulation by Urbano uses models that describe the data well (as shown in this thesis), then the results (regarding which statistical significance test is optimal) should also be accurate. So, same conclusions as in Urbano2019, which are: use t-test?
%	\end{itemize}
%
%\section{Future Work}
%	\begin{itemize}
%		\item Fix the simulation to better fit data where the number of zeros is high.
%		\item  Define a better $\Delta$(CDF, ECDF) metric, for the case of RR. Our measure leads to very small values, which might suggest that it is not well defined to handle those cases.
%	\end{itemize}