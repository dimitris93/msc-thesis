Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Urbano2018,
abstract = {Part of Information Retrieval evaluation research is limited by the fact that we do not know the distributions of system effectiveness over the populations of topics and, by extension, their true mean scores. The workaround usually consists in resampling topics from an existing collection and approximating the statistics of interest with the observations made between random subsamples, as if one represented the population and the other a random sample. However, this methodology is clearly limited by the availability of data, the impossibility to control the properties of these data, and the fact that we do not really measure what we intend to. To overcome these limitations, we propose a method based on vine copulas for stochastic simulation of evaluation results where the true system distributions are known upfront. In the basic use case, it takes the scores from an existing collection to build a semi-parametric model representing the set of systems and the population of topics, which can then be used to make realistic simulations of the scores by the same systems but on random new topics. Our ability to simulate this kind of data not only eliminates the current limitations, but also offers new opportunities for research. As an example, we show the benefits of this approach in two sample applications replicating typical experiments found in the literature. We provide a full R package to simulate new data following the proposed method, which can also be used to fully reproduce the results in this paper.},
address = {New York, NY, USA},
author = {Urbano, Juli{\'{a}}n and Nagler, Thomas},
booktitle = {The 41st International ACM SIGIR Conference on Research {\&} Development in Information Retrieval},
doi = {10.1145/3209978.3210043},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Nagler - 2018 - Stochastic Simulation of Test Collections Evaluation Scores.pdf:pdf},
isbn = {9781450356572},
keywords = {Copula,Distribution,Evaluation,Simulation,Test collection},
pages = {695--704},
series = {SIGIR '18},
title = {{Stochastic Simulation of Test Collections: Evaluation Scores}},
url = {https://dl.acm.org/doi/10.1145/3209978.3210043},
year = {2018}
}
@inproceedings{Urbano2021,
address = {New York, NY, USA},
author = {Urbano, Juli{\'{a}}n and Corsi, Matteo and Hanjalic, Alan},
booktitle = {Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
doi = {10.1145/3471158.3472242},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Corsi, Hanjalic - 2021 - How do Metric Score Distributions affect the Type I Error Rate of Statistical Significance Tests in Inf.pdf:pdf},
isbn = {9781450386111},
keywords = {2021,Simulation,Statistical significance,Type I error,acm reference format,and alan hanjalic,how do metric score,juli{\'{a}}n urbano,matteo corsi,simulation,skewness,statistical significance,type i error},
month = {jul},
pages = {245--250},
publisher = {ACM Press},
series = {ICTIR '21},
title = {{How do Metric Score Distributions affect the Type I Error Rate of Statistical Significance Tests in Information Retrieval?}},
url = {https://dl.acm.org/doi/10.1145/3471158.3472242},
year = {2021}
}
@book{Rijsbergen1979,
author = {van Rijsbergen, C. J.},
doi = {10.5555/539927},
edition = {2nd},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Rijsbergen - 1979 - Information Retrieval.pdf:pdf},
isbn = {9781538633748},
publisher = {Butterworth-Heinemann},
title = {{Information Retrieval}},
year = {1979}
}
@inproceedings{Carterette2009,
author = {Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A. and Allan, James},
booktitle = {Proceedings of the 31st European Conference on IR Research},
doi = {10.1007/978-3-642-00958-7_27},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Carterette et al. - 2009 - If I Had a Million Queries.pdf:pdf},
pages = {288--300},
series = {ECIR 2009},
title = {{If I Had a Million Queries}},
url = {http://link.springer.com/10.1007/978-3-642-00958-7{\_}27},
year = {2009}
}
@inproceedings{Carterette2015,
abstract = {A key component of experimentation in IR is statistical hypothesis testing, which researchers and developers use to make inferences about the effectiveness of their system relative to others. A statistical hypothesis test can tell us the likelihood that small mean differences in effectiveness (on the order of 5{\%}, say) is due to randomness or measurement error, and thus is critical for making progress in research. But the tests typically used in IR-the t-test, the Wilcoxon signed-rank test-are very general, not developed specifically for the problems we face in information retrieval evaluation. A better approach would take advantage of the fact that the atomic unit of measurement in IR is the relevance judgment rather than the effectiveness measure, and develop tests that model relevance directly. In this work we present such an approach, showing theoretically that modeling relevance in this way naturally gives rise to the effectiveness measures we care about. We demonstrate the usefulness of our model on both simulated data and a diverse set of runs from various TREC tracks.},
address = {New York, NY, USA},
author = {Carterette, Ben},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
doi = {10.1145/2808194.2809469},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Carterette - 2015 - Bayesian Inference for Information Retrieval Evaluation.pdf:pdf},
isbn = {9781450338332},
keywords = {Bayesian inference,Evaluation,Information retrieval,Statistical testing},
month = {sep},
pages = {31--40},
publisher = {ACM},
series = {ICTIR '15},
title = {{Bayesian Inference for Information Retrieval Evaluation}},
url = {https://dl.acm.org/doi/10.1145/2808194.2809469},
year = {2015}
}
@inproceedings{Azzopardi2007,
abstract = {There has been increased interest in the use of simulated queries for evaluation and estimation purposes in Information Retrieval. However, there are still many unaddressed issues regarding their usage and impact on evaluation because their quality, in terms of retrieval performance, is unlike real queries. In this paper, we focus on methods for building simulated known-item topics and explore their quality against real known-item topics. Using existing generation models as our starting point, we explore factors which may influence the generation of the known-item topic. Informed by this detailed analysis (on six European languages) we propose a model with improved document and term selection properties, showing that simulated known-item topics can be generated that are comparable to real known-item topics. This is a significant step towards validating the potential usefulness of simulated queries: for evaluation purposes, and because building models of querying behavior provides a deeper insight into the querying process so that better retrieval mechanisms can be developed to support the user.},
address = {New York, New York, USA},
author = {Azzopardi, Leif and de Rijke, Maarten and Balog, Krisztian},
booktitle = {Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/1277741.1277820},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Azzopardi, de Rijke, Balog - 2007 - Building Simulated Queries for Known-Item Topics.pdf:pdf},
isbn = {9781595935977},
keywords = {evaluation,multilin-,query generations,query simulation},
pages = {455--462},
publisher = {ACM Press},
series = {SIGIR '07},
title = {{Building Simulated Queries for Known-Item Topics}},
url = {http://portal.acm.org/citation.cfm?doid=1277741.1277820},
year = {2007}
}
@book{Box2005,
author = {Box, George Edward Pelham and Hunter, William Gordon and Hunter, John Stuart},
edition = {2nd},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Box, Hunter, Hunter - 2005 - Statistics for Experimenters.pdf:pdf},
isbn = {9780471093152},
publisher = {John Wiley {\&} Sons},
title = {{Statistics for Experimenters}},
year = {2005}
}
@article{Sanderson2010,
abstract = {Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. This monograph surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. This monograph reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern-day test collection is little different from the structures that the pioneering researchers in the 1950s and 1960s conceived of. This tutorial and review shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research. {\textcopyright} 2010 M. Sanderson.},
author = {Sanderson, Mark},
doi = {10.1561/1500000009},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Sanderson - 2010 - Test Collection Based Evaluation of Information Retrieval Systems.pdf:pdf},
issn = {1554-0669},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
number = {4},
pages = {247--375},
title = {{Test Collection Based Evaluation of Information Retrieval Systems}},
url = {http://www.nowpublishers.com/article/Details/INR-009},
volume = {4},
year = {2010}
}
@inproceedings{Baskaya2013,
address = {New York, New York, USA},
author = {Baskaya, Feza and Keskustalo, Heikki and J{\"{a}}rvelin, Kalervo},
booktitle = {Proceedings of the 22nd ACM international conference on Conference on information {\&} knowledge management},
doi = {10.1145/2505515.2505660},
isbn = {9781450322638},
pages = {2297--2302},
publisher = {ACM Press},
series = {CIKM '13},
title = {{Modeling behavioral factors ininteractive information retrieval}},
url = {http://dl.acm.org/citation.cfm?doid=2505515.2505660},
year = {2013}
}
@inproceedings{Robertson2012,
abstract = {We explore the notion, put forward by Cormack {\&} Lynam and Robertson, that we should consider a document collection used for Cranfield-style experiments as a sample from some larger population of documents. In this view, any per-topic metric (such as average precision) should be regarded as an estimate of that metric's true value for that topic in the full population, and therefore as carrying its own per-topic variance or estimate precision or noise. As in the two mentioned papers, we explore this notion by simulating other samples from the same large population. We investigate different ways of performing this simulation. One use of this analysis is to refine the notion of statistical significance of a difference between two systems (in most such analyses, each per-topic measurement is treated as equally precise). We propose a mixed-effects model method to measure significance, and compare it experimentally with the traditional t-test. {\textcopyright} 2012 ACM.},
address = {New York, New York, USA},
author = {Robertson, Stephen E. and Kanoulas, Evangelos},
booktitle = {Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/2348283.2348402},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Robertson, Kanoulas - 2012 - On per-topic variance in IR evaluation.pdf:pdf},
isbn = {9781450314725},
keywords = {evaluation,information retrieval,mixed-effects model,significance testing,simulation,statistical precision},
pages = {891--900},
publisher = {ACM Press},
series = {SIGIR '12},
title = {{On per-topic variance in IR evaluation}},
url = {http://dl.acm.org/citation.cfm?doid=2348283.2348402},
year = {2012}
}
@article{Anderson1952,
author = {Anderson, T. W. and Darling, D. A.},
doi = {10.1214/aoms/1177729437},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Anderson, Darling - 1952 - Asymptotic Theory of Certain Goodness of Fit Criteria Based on Stochastic Processes.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {2},
pages = {193--212},
title = {{Asymptotic Theory of Certain "Goodness of Fit" Criteria Based on Stochastic Processes}},
url = {http://projecteuclid.org/euclid.aoms/1177729437},
volume = {23},
year = {1952}
}
@article{Urbano2016,
abstract = {The number of topics that a test collection contains has a direct impact on how well the evaluation results reflect the true performance of systems. However, large collections can be prohibitively expensive, so researchers are bound to balance reliability and cost. This issue arises when researchers have an existing collection and they would like to know how much they can trust their results, and also when they are building a new collection and they would like to know how many topics it should contain before they can trust the results. Several measures have been proposed in the literature to quantify the accuracy of a collection to estimate the true scores, as well as different ways to estimate the expected accuracy of hypothetical collections with a certain number of topics. We can find ad-hoc measures such as Kendall tau correlation and swap rates, and statistical measures such as statistical power and indexes from generalizability theory. Each measure focuses on different aspects of evaluation, has a different theoretical basis, and makes a number of assumptions that are not met in practice, such as normality of distributions, homoscedasticity, uncorrelated effects and random sampling. However, how good these estimates are in practice remains a largely open question. In this paper we first compare measures and estimators of test collection accuracy and propose unbiased statistical estimators of the Kendall tau and tau AP correlation coefficients. Second, we detail a method for stochastic simulation of evaluation results under different statistical assumptions, which can be used for a variety of evaluation research where we need to know the true scores of systems. Third, through large-scale simulation from TREC data, we analyze the bias of a range of estimators of test collection accuracy. Fourth, we analyze the robustness to statistical assumptions of these estimators, in order to understand what aspects of an evaluation are affected by what assumptions and guide in the development of new collections and new measures. All the results in this paper are fully reproducible with data and code available online.},
author = {Urbano, Juli{\'{a}}n},
doi = {10.1007/s10791-015-9274-y},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano - 2016 - Test Collection Reliability A Study of Bias and Robustness to Statistical Assumptions via Stochastic Simulation.pdf:pdf},
issn = {1573-7659},
journal = {Information Retrieval Journal},
keywords = {Evaluation,Information retrieval,Reliability,Simulation,Test collection},
month = {jun},
number = {3},
pages = {313--350},
title = {{Test Collection Reliability: A Study of Bias and Robustness to Statistical Assumptions via Stochastic Simulation}},
url = {http://link.springer.com/10.1007/s10791-015-9274-y},
volume = {19},
year = {2016}
}
@inproceedings{Urbano2013a,
abstract = {Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5{\%}, further confirming that we may actually be underestimating significance. Copyright {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Urbano, Juli{\'{a}}n and Marrero, M{\'{o}}nica and Mart{\'{i}}n, Diego},
booktitle = {Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/2484028.2484163},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Marrero, Mart{\'{i}}n - 2013 - A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation.pdf:pdf;:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Marrero, Mart{\'{i}}n - 2013 - A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation.webp:webp},
isbn = {9781450320344},
keywords = {Bootstrap,Evaluation,Permutation,Randomization,Sign test,Statistical significance,Student's t-test,Wilcoxon test},
month = {jul},
pages = {925--928},
publisher = {ACM Press},
series = {SIGIR '13},
title = {{A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation}},
url = {https://dl.acm.org/doi/10.1145/2484028.2484163},
year = {2013}
}
@inproceedings{Voorhees2002,
abstract = {Retrieval mechanisms are frequently compared by computing the respective average scores for some effectiveness metric across a common set of information needs or topics, with researchers concluding one method is superior based on those averages. Since comparative retrieval system behavior is known to be highly variable across topics, good experimental design requires that a "sufficient" number of topics be used in the test. This paper uses TREC results to empirically derive error rates based on the number of topics used in a test and the observed difference in the average scores. The error rates quantify the likelihood that a different set of topics of the same size would lead to a different conclusion. We directly compute error rates for topic sets up to size 25, and extrapolate those rates for larger topic set sizes. The error rates found are larger than anticipated, indicating researchers need to take care when concluding one method is better than another, especially if few topics are used.},
address = {New York, New York, USA},
author = {Voorhees, Ellen M. and Buckley, Chris},
booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/564376.564432},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Voorhees, Buckley - 2002 - The Effect of Topic Set Size on Retrieval Experiment Error.pdf:pdf},
isbn = {1581135610},
issn = {01635840},
keywords = {Measurement error,Test collections},
pages = {316--323},
publisher = {ACM Press},
series = {SIGIR '02},
title = {{The Effect of Topic Set Size on Retrieval Experiment Error}},
url = {http://portal.acm.org/citation.cfm?doid=564376.564432},
year = {2002}
}
@inproceedings{Hull1993,
abstract = {The standard strategies for evaluation based on precision and recall are examined and their relative advantages and disadvantages are discussed. In particular, it is suggested that relevance feedback be evaluated from the perspective of the user. A number of different statistical tests are described for determining if differences in performance between retrieval methods are significant. These tests have often been ignored in the past because most are based on an assumption of normality which is not strictly valid for the standard performance measures. However, one can test this assumption using simple diagnostic plots, and if it is a poor approximation, there are a number of non-parametric alternatives.},
address = {New York, NY, USA},
author = {Hull, David},
booktitle = {Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/160688.160758},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Hull - 1993 - Using Statistical Testing in the Evaluation of Retrieval Experiments.pdf:pdf},
isbn = {0897916050},
pages = {329--338},
publisher = {ACM Press},
series = {SIGIR '93},
title = {{Using Statistical Testing in the Evaluation of Retrieval Experiments}},
url = {http://portal.acm.org/citation.cfm?doid=160688.160758},
year = {1993}
}
@article{Kolmogorov1933,
abstract = {Normal Distribution},
author = {Kolmogorov, A N},
journal = {Giorn. Inst. Italiano Attuari},
pages = {83--91},
title = {{Sulla determinazione empirica di una legge di distribuzione}},
volume = {4},
year = {1933}
}
@article{Akaike1974,
author = {Akaike, H.},
doi = {10.1109/TAC.1974.1100705},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Akaike - 1974 - A New Look at the Statistical Model Identification.pdf:pdf},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
title = {{A New Look at the Statistical Model Identification}},
url = {http://ieeexplore.ieee.org/document/1100705/},
volume = {19},
year = {1974}
}
@article{Cooper1973,
author = {Cooper, Michael D},
doi = {10.1016/0020-0271(73)90004-1},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Cooper - 1973 - A simulation model of an information retrieval system.pdf:pdf},
issn = {00200271},
journal = {Information Storage and Retrieval},
month = {jan},
number = {1},
pages = {13--32},
title = {{A simulation model of an information retrieval system}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0020027173900041},
volume = {9},
year = {1973}
}
@inproceedings{Sakai2016,
abstract = {There are two well-known versions of the t-test for comparing means from unpaired data: Student's t-test and Welch's t-test. While Welch's t-test does not assume homoscedasticity (i.e., equal variances), it involves approximations. A classical textbook recommendation would be to use Student's t-test if either the two sample sizes are similar or the two sample variances are similar, and to use Welch's t-test only when both of the above conditions are violated. However, a more recent recommendation seems to be to use Welch's t-test unconditionally. Using past data from both TREC and NTCIR, the present study demonstrates that the latter advice should not be followed blindly in the context of IR system evaluation. More specifically, our results suggest that if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch's t-test may not be reliable.},
address = {New York, NY, USA},
author = {Sakai, Tetsuya},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},
doi = {10.1145/2911451.2914684},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Sakai - 2016 - Two Sample T-tests for IR Evaluation Student or Welch.pdf:pdf},
isbn = {9781450340694},
keywords = {Statistical significance,Test collections,Topics,Variances},
month = {jul},
pages = {1045--1048},
publisher = {ACM Press},
series = {SIGIR '16},
title = {{Two Sample T-tests for IR Evaluation: Student or Welch?}},
url = {https://dl.acm.org/doi/10.1145/2911451.2914684},
year = {2016}
}
@article{Schwarz1978,
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
title = {{Estimating the Dimension of a Model}},
url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
volume = {6},
year = {1978}
}
@inproceedings{Parapar2021,
abstract = {Null Hypothesis Significance Testing (NHST) has been recurrently employed as the reference framework to assess the difference in performance between Information Retrieval (IR) systems. IR practitioners customarily apply significance tests, such as the t-test, the Wilcoxon Signed Rank test, the Permutation test, the Sign test or the Bootstrap test. However, the question of which of these tests is the most reliable in IR experimentation is still controversial. Different authors have tried to shed light on this issue, but their conclusions are not in agreement. In this paper, we present a new methodology for assessing the behavior of significance tests in typical ranking tasks. Our method creates models from the search systems and uses those models to simulate different inputs to the significance tests. With such an approach, we can control the experimental conditions and run experiments with full knowledge about the truth or falseness of the null hypothesis. Following our methodology, we computed a series of simulations that estimate the proportion of Type I and Type II errors made by different tests. Results conclusively suggest that the Wilcoxon test is the most reliable test and, thus, IR practitioners should adopt it as the reference tool to assess differences between IR systems.},
address = {New York, NY, USA},
author = {Parapar, Javier and Losada, David E. and Barreiro, {\'{A}}lvaro},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
doi = {10.1145/3412841.3441945},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Parapar, Losada, Barreiro - 2021 - Testing the Tests Simulation of Rankings to Compare Statistical Significance Tests in Information Ret.pdf:pdf},
isbn = {9781450381048},
keywords = {information retrieval,simulation,statistical testing},
pages = {655--664},
publisher = {ACM Press},
series = {SAC '21},
title = {{Testing the Tests: Simulation of Rankings to Compare Statistical Significance Tests in Information Retrieval Evaluation}},
url = {https://dl.acm.org/doi/10.1145/3412841.3441945},
year = {2021}
}
@inproceedings{Cormack2007,
abstract = {We examine the validity and power of the t-test, Wilcoxon test, and sign test in determining whether or not the difference in performance between two IR systems is significant. Empirical tests conducted on subsets of the TREC2004 Robust Retrieval collection indicate that the p-values computed by these tests for the difference in mean average precision (MAP) between two systems are very accurate fora wide range of sample sizes and significance estimates. Similarly, these tests have good power, with the t-test proving superior overall. The t-test is also valid for comparing geometric mean average precision (GMAP), exhibiting slightly superior accuracy and slightly inferior power than for MAPcomparison.},
address = {New York, NY, USA},
author = {Cormack, Gordon V. and Lynam, Thomas R.},
booktitle = {Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/1277741.1277892},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Cormack, Lynam - 2007 - Validity and Power of t-Test for Comparing MAP and GMAP.pdf:pdf},
isbn = {9781595935977},
keywords = {Significance test,Statistical power,Validity},
pages = {753},
publisher = {ACM Press},
series = {SIGIR '07},
title = {{Validity and Power of t-Test for Comparing MAP and GMAP}},
url = {http://portal.acm.org/citation.cfm?doid=1277741.1277892},
year = {2007}
}
@inproceedings{Tague1980,
author = {Tague, Jean and Nelson, Michael and Wu, Harry},
booktitle = {Proceedings of the 3rd Annual ACM Conference on Research and Development in Information Retrieval},
doi = {10.5555/636669.636684},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Tague, Nelson, Wu - 1980 - Problems in the Simulation of Bibliographic Retrieval Systems.pdf:pdf},
isbn = {0408107758},
pages = {236--255},
publisher = {Butterworth {\&} Co.},
series = {SIGIR '80},
title = {{Problems in the Simulation of Bibliographic Retrieval Systems}},
url = {https://dl.acm.org/doi/10.5555/636669.636684},
year = {1980}
}
@article{Wilbur1994,
abstract = {It has been observed that statistical tests are infre quently applied in analysing differences in the performance of different retrieval methods. We believe this is explained on the one hand by the complexity of the subject, and on the other hand by the desire to avoid misleading conclusions. Because practical retrieval methods cannot be explained by simple models, parametric statistical tests are generally not suitable. Some non-parametric tests require a symmetry in the null hypothesis that seems inappropriate to the required task. A second class of non-parametric tests comprise the bootstrap methods. Here, the null hypothesis seems appro priate to practical testing, but the bootstrap assumption (that the sample may adequately represent the whole population) may be in question. If the bootstrap assumption is false, one may be led to erroneous conclusions (type I or type II errors). Here, by use of a mathematical model [11] which approxi mates the behaviour of practical retrieval systems, we show that bootstrap methods perform well in performance com parisons based on actual test sets used in practice. Type I error is appropriately predictable and the power loss of the tests, when compared with the theoretically most power ful test in the most realistic setting, may not exceed ten per centage points. We conclude that the bootstrap methods provide a practical approach to statistical testing in the field of retrieval performance analysis.},
author = {Wilbur, W. John},
doi = {10.1177/016555159402000405},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Wilbur - 1994 - Non-parametric significance tests of retrieval performance comparisons.pdf:pdf},
issn = {0165-5515},
journal = {Journal of Information Science},
number = {4},
pages = {270--284},
title = {{Non-parametric significance tests of retrieval performance comparisons}},
url = {http://journals.sagepub.com/doi/10.1177/016555159402000405},
volume = {20},
year = {1994}
}
@article{Azzopardi2011,
abstract = {All search in the real-world is inherently interactive. Information retrieval (IR) has a firm tradition of using simulation to evaluate IR systems as embodied by the Cranfield paradigm. However, to a large extent, such system evaluations ignore user interaction. Simulations provide a way to go beyond this limitation. With an increasing number of researchers using simulation to evaluate interactive IR systems, it is now timely to discuss, develop and advance this powerful methodology within the field of IR. During the SimInt 2010 workshop around 40 participants discussed and presented their views on the simulation of interaction. The main conclusion and general consensus was that simulation offers great potential for the field of IR; and that simulations of user interaction can make explicit the user and the user interface while maintaining the advantages of the Cranfield paradigm.},
author = {Azzopardi, Leif and J{\"{a}}rvelin, Kalervo and Kamps, Jaap and Smucker, Mark D.},
doi = {10.1145/1924475.1924484},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Azzopardi et al. - 2011 - Report on the SIGIR 2010 Workshop on the Simulation of Interaction.pdf:pdf},
issn = {0163-5840},
journal = {ACM SIGIR Forum},
month = {jan},
number = {2},
pages = {35--47},
title = {{Report on the SIGIR 2010 Workshop on the Simulation of Interaction}},
url = {https://dl.acm.org/doi/10.1145/1924475.1924484},
volume = {44},
year = {2011}
}
@inproceedings{Manmatha2001,
abstract = {In this paper the score distributions of a number of text search engines are modeled. It is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. Experiments show that this model fits TREC-3 and TREC-4 data for not only probabilistic search engines like INQUERY but also vector space search engines like SMART for English. We have also used this model to fit the output of other search engines like LSI search engines and search engines indexing other languages like Chinese. It is then shown that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. We also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents. We hypothesize that all 'good' text search engines operating on any language have similar characteristics. This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques.},
address = {New York, New York, USA},
author = {Manmatha, R. and Rath, T. and Feng, F.},
booktitle = {Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/383952.384005},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Manmatha, Rath, Feng - 2001 - Modeling Score Distributions for Combining the Outputs of Search Engines.pdf:pdf},
isbn = {1581133316},
issn = {01635840},
pages = {267--275},
publisher = {ACM Press},
series = {SIGIR '01},
title = {{Modeling Score Distributions for Combining the Outputs of Search Engines}},
url = {http://portal.acm.org/citation.cfm?doid=383952.384005},
year = {2001}
}
@article{Smirnoff1939,
author = {Smirnoff, N.},
journal = {Matematicheskii Sbornik},
number = {1},
pages = {3--26},
title = {{Sur les écarts de la courbe de distribution empirique}},
volume = {48},
year = {1939}
}
@article{Parapar2020,
abstract = {In this article, we use innovative full-text citation analysis along with supervised topic modeling and network-analysis algorithms to enhance classical bibliometric analysis and publication/author/venue ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author-contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is calculated by PageRank with edge and vertex prior distributions. To evaluate this work, we sampled 104 topics (labeled with keywords) in review papers. The cited publications of each review paper are assumed to be “important publications” for the target topic (keyword), and we use these cited publications to validate our topic-ranking result and to compare different publication-ranking lists. Evaluation results show that full-text citation and publication content prior topic distribution, along with the classical PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance, comparing with term frequency–inverted document frequency (tf–idf), language model, BM25, PageRank, and PageRank + language model (p {\textless} .001), for academic information retrieval (IR) systems.},
author = {Parapar, Javier and Losada, David E. and Presedo‐Quindimil, Manuel A. and Barreiro, Alvaro},
doi = {10.1002/asi.24203},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Parapar et al. - 2020 - Using Score Distributions to Compare Statistical Significance Tests for Information Retrieval Evaluation.pdf:pdf},
issn = {2330-1635},
journal = {Journal of the Association for Information Science and Technology},
month = {jan},
number = {1},
pages = {98--113},
title = {{Using Score Distributions to Compare Statistical Significance Tests for Information Retrieval Evaluation}},
url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.24203},
volume = {71},
year = {2020}
}
@inproceedings{Smucker2007,
abstract = {Information retrieval (IR) researchers commonly use three tests of statistical significance: the Student's paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher's randomization (permutation) test as non- parametric significance tests for IR but these tests have seen little use. For each of these five tests, we took the ad-hoc retrieval runs submitted to TRECs 3 and 5-8, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means. Copyright 2007 ACM.},
address = {New York, NY, USA},
author = {Smucker, Mark D. and Allan, James and Carterette, Ben},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
doi = {10.1145/1321440.1321528},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Smucker, Allan, Carterette - 2007 - A Comparison of Statistical Significance Tests for Information Retrieval Evaluation.pdf:pdf},
isbn = {9781595938039},
keywords = {Bootstrap,Hypothesis test,Permutation,Randomization,Sign,Statistical significance,Student's t-test,Wilcoxon},
pages = {623--632},
publisher = {ACM Press},
series = {CIKM '07},
title = {{A Comparison of Statistical Significance Tests for Information Retrieval Evaluation}},
url = {http://portal.acm.org/citation.cfm?doid=1321440.1321528},
year = {2007}
}
@inproceedings{Sakai2006,
abstract = {This paper describes how the Bootstrap approach to statistics can be applied to the evaluation of IR effectiveness metrics. First, we argue that Bootstrap Hypothesis Tests deserve more attention from the IR community, as they are based on fewer assumptions than traditional statistical significance tests. We then describe straightforward methods for comparing the sensitivity of IR metrics based on Bootstrap Hypothesis Tests. Unlike the heuristics-based "swap" method proposed by Voorhees and Buckley, our method estimates the performance difference required to achieve a given significance level directly from Bootstrap Hypothesis Test results. In addition, we describe a simple way of examining the accuracy of rank correlation between two metrics based on the Bootstrap Estimate of Standard Error. We demonstrate the usefulness of our methods using test collections and runs from the NTCIR CLIR track for comparing seven IR metrics, including those that can handle graded relevance and those based on the Geometric Mean.},
address = {New York, NY, USA},
author = {Sakai, Tetsuya},
booktitle = {Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/1148170.1148261},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Sakai - 2006 - Evaluating Information Retrieval Metrics Based on Bootstrap.pdf:pdf},
isbn = {1595933697},
pages = {525--532},
publisher = {ACM Press},
series = {SIGIR '06},
title = {{Evaluating Information Retrieval Metrics Based on Bootstrap}},
url = {http://portal.acm.org/citation.cfm?doid=1148170.1148261},
year = {2006}
}
@book{VonMises1928,
address = {Berlin, Heidelberg},
author = {von Mises, Richard},
doi = {10.1007/978-3-662-36230-3},
isbn = {978-3-662-35402-5},
publisher = {Springer Berlin Heidelberg},
title = {{Wahrscheinlichkeit Statistik und Wahrheit}},
url = {http://link.springer.com/10.1007/978-3-662-36230-3},
year = {1928}
}
@article{White2005,
abstract = {In this article we describe an evaluation of relevance feedback (RF) algorithms using searcher simulations. Since these algorithms select additional terms for query modification based on inferences made from searcher interaction, not on relevance information searchers explicitly provide (as in traditional RF), we refer to them as implicit feedback models . We introduce six different models that base their decisions on the interactions of searchers and use different approaches to rank query modification terms. The aim of this article is to determine which of these models should be used to assist searchers in the systems we develop. To evaluate these models we used searcher simulations that afforded us more control over the experimental conditions than experiments with human subjects and allowed complex interaction to be modeled without the need for costly human experimentation. The simulation-based evaluation methodology measures how well the models learn the distribution of terms across relevant documents (i.e., learn what information is relevant) and how well they improve search effectiveness (i.e., create effective search queries). Our findings show that an implicit feedback model based on Jeffrey's rule of conditioning outperformed other models under investigation.},
author = {White, Ryen W. and Ruthven, Ian and Jose, Joemon M. and Rijsbergen, C. J. Van},
doi = {10.1145/1080343.1080347},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/White et al. - 2005 - Evaluating implicit feedback models using searcher simulations.pdf:pdf},
issn = {1046-8188},
journal = {ACM Transactions on Information Systems},
keywords = {Evaluation,Implicit feedback,Relevance feedback,User simulations},
month = {jul},
number = {3},
pages = {325--361},
title = {{Evaluating implicit feedback models using searcher simulations}},
url = {https://dl.acm.org/doi/10.1145/1080343.1080347},
volume = {23},
year = {2005}
}
@article{Cramer1928,
author = {Cram{\'{e}}r, Harald},
doi = {10.1080/03461238.1928.10416862},
issn = {0346-1238},
journal = {Scandinavian Actuarial Journal},
number = {1},
pages = {13--74},
publisher = {Taylor {\&} Francis},
title = {{On the composition of elementary errors: First paper: Mathematical deductions}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862},
volume = {1928},
year = {1928}
}
@inproceedings{Zobel1998,
abstract = {Two stages in measurement of techniques for information retrieval are gathering of documents for relevance assessment and use of the assessments to numerically evaluate effectiveness. We consider both of these stages in the context of the TREC experiments, to determine whether they lead to measurements that are trustworthy and fair. Our detailed empirical investigation of the TREC results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found. We propose a new pooling strategy that can significantly increase the number of relevant documents found for given effort, without compromising fairness.},
address = {New York, NY, USA},
author = {Zobel, Justin},
booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/290941.291014},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Zobel - 1998 - How Reliable are the Results of Large-Scale Information Retrieval Experiments.pdf:pdf},
isbn = {1581130155},
pages = {307--314},
publisher = {ACM Press},
series = {SIGIR '98},
title = {{How Reliable are the Results of Large-Scale Information Retrieval Experiments?}},
url = {http://portal.acm.org/citation.cfm?doid=290941.291014},
year = {1998}
}
@article{White2006,
abstract = {Information seeking is traditionally conducted in environments where search results are represented at the user interface by a minimal amount of meta-information such as titles and query-based summaries. The goal of this form of presentation is to give searchers sufficient context to help them make informed interaction decisions without overloading them cognitively. The principle of polyrepresentation [Ingwersen, P. (1996). Cognitive perspectives of information retrieval interaction: elements of a cognitive IR theory. Journal of Documentation 52, 3-50] suggests that information retrieval (IR) systems should provide and use different cognitive structures during acts of communication to reduce the uncertainty associated with interactive IR. In previous work we have created content-rich search interfaces that implement an aspect of polyrepresentative theory, and are capable of displaying multiple representations of the retrieved documents simultaneously at the results interface. Searcher interaction with content-rich interfaces was used as implicit relevance feedback (IRF) to construct modified queries. These interfaces have been shown to be successful in experimentation with human subjects but we do not know whether the information was presented in a way that makes good use of the display space, or positioned most useful components in easily accessible locations, for use in IRF. In this article we use simulations of searcher interaction behaviour as design tools to determine the most rational interface design for when IRF is employed. This research forms part of the iterative design of interfaces to proactively support searchers. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {White, Ryen W.},
doi = {10.1016/j.ipm.2006.02.005},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/White - 2006 - Using searcher simulations to redesign a polyrepresentative implicit feedback interface.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
keywords = {Implicit relevance feedback,Interface design,Polyrepresentation,Searcher simulations},
month = {sep},
number = {5},
pages = {1185--1202},
title = {{Using searcher simulations to redesign a polyrepresentative implicit feedback interface}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457306000173},
volume = {42},
year = {2006}
}
@inproceedings{Urbano2019,
abstract = {Statistical significance testing is widely accepted as a means to assess how well a difference in effectiveness reflects an actual difference between systems, as opposed to random noise because of the selection of topics. According to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is the most popular choice among IR researchers. However, previous work has suggested computer intensive tests like the bootstrap or the permutation test, based mainly on theoretical arguments. On empirical grounds, others have suggested non-parametric alternatives such as the Wilcoxon test. Indeed, the question of which tests we should use has accompanied IR and related fields for decades now. Previous theoretical studies on this matter were limited in that we know that test assumptions are not met in IR experiments, and empirical studies were limited in that we do not have the necessary control over the null hypotheses to compute actual Type I and Type II error rates under realistic conditions. Therefore, not only is it unclear which test to use, but also how much trust we should put in them. In contrast to past studies, in this paper we employ a recent simulation methodology from TREC data to go around these limitations. Our study comprises over 500 million p-values computed for a range of tests, systems, effectiveness measures, topic set sizes and effect sizes, and for both the 2-tail and 1-tail cases. Having such a large supply of IR evaluation data with full knowledge of the null hypotheses, we are finally in a position to evaluate how well statistical significance tests really behave with IR data, and make sound recommendations for practitioners.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1905.11096},
author = {Urbano, Juli{\'{a}}n and Lima, Harlley and Hanjalic, Alan},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.5555/539927},
eprint = {1905.11096},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Lima, Hanjalic - 2019 - Statistical Significance Testing in Information Retrieval An Empirical Analysis of Type I, Type II and T.pdf:pdf;:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Urbano, Lima, Hanjalic - 2019 - Statistical Significance Testing in Information Retrieval An Empirical Analysis of Type I, Type II an(2).pdf:pdf},
isbn = {0408709294},
keywords = {Bootstrap,Permutation,Sign test,Simulation,Statistical significance,Student's t-test,Type I,Type II errors,Wilcoxon test},
pages = {505--514},
publisher = {ACM Press},
series = {SIGIR '19},
title = {{Statistical Significance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors}},
url = {https://dl.acm.org/doi/10.1145/3331184.3331259},
year = {2019}
}
@inproceedings{Bodoff2007,
abstract = {How good is an IR test collection? A series of papers in recent years has addressed the question by empirically enumerating the consistency of performance comparisons using alternate subsets of the collection. In this paper we propose using Test Theory, which is based on analysis of variance and is specifically designed to assess test collections. Using the method, we not only can measure test reliability after the fact, but we can estimate the test collection's reliability before it is even built or used. We can also determine an optimal allocation of resources before the fact, e.g. whether to invest in more judges or queries. The method, which is in widespread use in the field of educational testing, complements data-driven approaches to assessing test collections. Whereas the data-driven method focuses on test results, test theory focuses on test designs. It offers unique practical results, as well as insights about the variety and implications of alternative test designs. Copyright 2007 ACM.},
address = {New York, New York, USA},
author = {Bodoff, David and Li, Pu},
booktitle = {Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/1277741.1277805},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Bodoff, Li - 2007 - Test Theory for Assessing IR Test Collections.pdf:pdf},
isbn = {9781595935977},
keywords = {Information retrieval,Test collections,Test theory},
number = {July},
pages = {367--374},
publisher = {ACM Press},
series = {SIGIR '07},
title = {{Test Theory for Assessing IR Test Collections}},
url = {http://portal.acm.org/citation.cfm?doid=1277741.1277805},
year = {2007}
}
@inproceedings{Sanderson2005,
abstract = {The effectiveness of information retrieval systems is measured by comparing performance on a common set of queries and documents. Significance tests are often used to evaluate the reliability of such comparisons. Previous work has examined such tests, but produced results with limited application. Other work established an alternative benchmark for significance, but the resulting test was too stringent. In this paper, we revisit the question of how such tests should be used. We find that the t-test is highly reliable (more so than the sign or Wilcoxon test), and is far more reliable than simply showing a large percentage difference in effectiveness measures between IR systems. Our results show that past empirical work on significance tests over-estimated the error of such tests. We also re-consider comparisons between the reliability of precision at rank 10 and mean average precision, arguing that past comparisons did not consider the assessor effort required to compute such measures. This investigation shows that assessor effort would be better spent building test collections with more topics, each assessed in less detail. {\textcopyright} 2005 ACM.},
address = {New York, NY, USA},
author = {Sanderson, Mark and Zobel, Justin},
booktitle = {Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/1076034.1076064},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Sanderson, Zobel - 2005 - Information Retrieval System Evaluation Effort, Sensitivity, and Reliability.pdf:pdf},
isbn = {1595930345},
keywords = {mean average precision,precision at 10,significance tests},
number = {1979},
pages = {162--169},
publisher = {ACM Press},
series = {SIGIR '05},
title = {{Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability}},
url = {http://dl.acm.org/citation.cfm?doid=1076034.1076064},
year = {2005}
}
@book{Sklar1959,
author = {Sklar, Abe},
pages = {229--231},
publisher = {Publications de l'Institut de statistique de l'Universit{\'{e}} de Paris},
title = {{Fonctions de R{\'{e}}partition {\`{a}} n Dimensions et Leurs Marges}},
volume = {8},
year = {1959}
}
@inproceedings{Roitero2019,
abstract = {Recently proposed methods allow the generation of simulated scores representing the values of an effectiveness metric, but they do not investigate the generation of the actual lists of retrieved documents. In this paper we address this limitation: we present an approach that exploits an evolutionary algorithm and, given a metric score, creates a simulated relevance profile (i.e., a ranked list of relevance values) that produces that score. We show how the simulated relevance profiles are realistic under various analyses.},
address = {New York, NY, USA},
author = {Roitero, Kevin and Brunello, Andrea and Urbano, Juli{\'{a}}n and Mizzaro, Stefano},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3357384.3358123},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Roitero et al. - 2019 - Towards Stochastic Simulations of Relevance Profiles.pdf:pdf},
isbn = {9781450369763},
keywords = {Genetic algorithms,Stochastic simulations,Test collections},
month = {nov},
pages = {2217--2220},
publisher = {ACM},
series = {CIKM '19},
title = {{Towards Stochastic Simulations of Relevance Profiles}},
url = {https://dl.acm.org/doi/10.1145/3357384.3358123},
year = {2019}
}
@article{Carterette2012,
abstract = {High-quality reusable test collections and formal statistical hypothesis testing together support a rigorous experimental environment for information retrieval research. But as Armstrong et al. [2009b] recently argued, global analysis of experiments suggests that there has actually been little real improvement in ad hoc retrieval effectiveness over time. We investigate this phenomenon in the context of simultaneous testing of many hypotheses using a fixed set of data. We argue that the most common approaches to significance testing ignore a great deal of information about the world. Taking into account even a fairly small amount of this information can lead to very different conclusions about systems than those that have appeared in published literature. We demonstrate how to model a set of IR experiments for analysis both mathematically and practically, and show that doing so can cause p -values from statistical hypothesis tests to increase by orders of magnitude. This has major consequences on the interpretation of experimental results using reusable test collections: it is very difficult to conclude that anything is significant once we have modeled many of the sources of randomness in experimental design and analysis.},
author = {Carterette, Benjamin A.},
doi = {10.1145/2094072.2094076},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Carterette - 2012 - Multiple testing in statistical analysis of systems-based information retrieval experiments.pdf:pdf},
issn = {1046-8188},
journal = {ACM Transactions on Information Systems},
keywords = {Effectiveness evaluation,Experimental design,Information retrieval,Statistical analysis,Test collections},
month = {feb},
number = {1},
pages = {1--34},
title = {{Multiple testing in statistical analysis of systems-based information retrieval experiments}},
url = {https://dl.acm.org/doi/10.1145/2094072.2094076},
volume = {30},
year = {2012}
}
@book{Good2005,
author = {Good, Phillip},
doi = {10.1007/b138696},
edition = {3},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Good - 2005 - Permutation, Parametric and Bootstrap Tests of Hypotheses.pdf:pdf},
isbn = {0-387-20279-X},
publisher = {Springer},
series = {Springer Series in Statistics},
title = {{Permutation, Parametric and Bootstrap Tests of Hypotheses}},
url = {http://link.springer.com/10.1007/b138696},
year = {2005}
}
@inproceedings{Maxwell2016,
abstract = {Most of the current models that are used to simulate users in Interactive Information Retrieval (IIR) lack realism and agency. Such models generally make decisions in a stochastic manner, without recourse to the actual information encountered or the underlying information need. In this paper, we develop a more sophisticated model of the user that includes their cognitive state within the simulation. The cognitive state maintains data about what the simulated user knows, has done and has seen, along with representations of what it considers attractive and relevant. Decisions to inspect or judge are then made based upon the simulated user's current state, rather than stochastically. In the context of ad-hoc topic retrieval, we evaluate the quality of the simulated users and agents by comparing their behaviour and performance against 48 human subjects under the same conditions, topics, time constraints, costs and search engine. Our findings show that while na{\"{i}}ve configurations of simulated users and agents substantially outperform our human subjects, their search behaviour is notably different from actual searchers. However, more sophisticated search agents can be tuned to act more like actual searchers providing greater realism. This innovation advances the state of the art in simulation, from simulated users towards autonomous agents. It provides a much needed step forward enabling the creation of more realistic simulations, while also motivating the development of more advanced cognitive agents and tools to help support and augment human searchers. Future work will focus not only on the pragmatics of tuning and training such agents for topic retrieval, but will also look at developing agents for other tasks and contexts such as collaborative search and slow search.},
address = {New York, NY, USA},
author = {Maxwell, David and Azzopardi, Leif},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
doi = {10.1145/2983323.2983805},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Maxwell, Azzopardi - 2016 - Agents, Simulated Users and Humans An Analysis of Performance and Behaviour.pdf:pdf},
isbn = {9781450340731},
month = {oct},
pages = {731--740},
publisher = {ACM},
series = {CIKM '16},
title = {{Agents, Simulated Users and Humans: An Analysis of Performance and Behaviour}},
url = {https://dl.acm.org/doi/10.1145/2983323.2983805},
year = {2016}
}
@inproceedings{Carterette2017,
abstract = {We analyze 5,792 IR conference papers published over 20 years to investigate how researchers have used and are using statistical significance testing in their experiments.},
address = {New York, NY, USA},
author = {Carterette, Ben},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3077136.3080738},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Carterette - 2017 - But Is It Statistically Significant Statistical Significance in IR Research, 1995-2014.pdf:pdf},
isbn = {9781450350228},
month = {aug},
pages = {1125--1128},
publisher = {ACM Press},
series = {SIGIR '17},
title = {{But Is It Statistically Significant?: Statistical Significance in IR Research, 1995-2014}},
url = {https://dl.acm.org/doi/10.1145/3077136.3080738},
year = {2017}
}
@article{Savoy1997,
abstract = {Evaluation methodology, and particularly its statistical tests associated, plays a central role in the information retrieval domain which maintains a strong empirical tradition. In an effort to evaluate the retrieval effectiveness of a search algorithm, this paper focuses on the average precision over a set of fixed recall values. After reviewing traditional evaluation methodology through the use of examples, this study suggests applying another statistical inference methodology called bootstrap, within which no particular assumption is needed about the distribution of the observations. Moreover, this scheme may be used to assert the accuracy of virtually any statistic, to build approximate confidence interval, and to verify whether a statistically significant difference exists between two retrieval schemes, even when dealing with a relatively small sample size. This study also suggests selecting the sample median rather than the sample mean in evaluating retrieval effectiveness where the justification for this choice is based on the nature of the information retrieval data. {\textcopyright} 1997 Elsevier Science Ltd.},
author = {Savoy, Jacques},
doi = {10.1016/S0306-4573(97)00027-7},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Savoy - 1997 - Statistical inference in retrieval effectiveness evaluation.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
number = {4},
pages = {495--512},
title = {{Statistical inference in retrieval effectiveness evaluation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457397000277},
volume = {33},
year = {1997}
}
@article{Nagler2018,
abstract = {We describe the R package kdecopula (current version 0.9.2), which provides fast implementations of various kernel estimators for the copula density. Due to a variety of available plotting options it is particularly useful for the exploratory analysis of dependence structures. It can be further used for accurate nonparametric estimation of copula densities and resampling. The implementation features spline interpolation of the estimates to allow for fast evaluation of density estimates and integrals thereof. We utilize this for a fast renormalization scheme that ensures that estimates are bona fide copula densities and additionally improves the estimators' accuracy. The performance of the methods is illustrated by simulations.},
archivePrefix = {arXiv},
arxivId = {1603.04229},
author = {Nagler, Thomas},
doi = {10.18637/jss.v084.i07},
eprint = {1603.04229},
file = {:C$\backslash$:/Users/Dimitris/Desktop/STUDY/Mendeley/papers/Nagler - 2018 - kdecopula An R Package for the Kernel Estimation of Bivariate Copula Densities.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Copula,Dependence,Exploratory data analysis,Kernel density,Nonparametric,R},
number = {7},
pages = {1--21},
title = {{kdecopula : An R Package for the Kernel Estimation of Bivariate Copula Densities}},
url = {http://www.jstatsoft.org/v84/i07/},
volume = {84},
year = {2018}
}
@book{Joe2014,
abstract = {Dependence Modeling with Copulas covers the substantial advances that have taken place in the field during the last 15 years, including vine copula modeling of high-dimensional data. Vine copula models are constructed from a sequence of bivariate copulas. The book develops generalizations of vine copula models, including common and structured factor models that extend from the Gaussian assumption to copulas. It also discusses other multivariate constructions and parametric copula families that have different tail properties and presents extensive material on dependence and tail properties to assist in copula model selection. The author shows how numerical methods and algorithms for inference and simulation are important in high-dimensional copula applications. He presents the algorithms as pseudocode, illustrating their implementation for high-dimensional copula models. He also incorporates results to determine dependence and tail properties of multivariate distributions for future constructions of copula models.},
author = {Joe, Harry},
booktitle = {Dependence Modeling with Copulas},
doi = {10.1201/b17116},
isbn = {9781466583238},
month = {jun},
publisher = {Chapman and Hall/CRC},
title = {{Dependence Modeling with Copulas}},
url = {https://www.taylorfrancis.com/books/9781466583238},
year = {2014}
}
@inproceedings{Sakai2016b,
abstract = {We conducted a systematic review of 840 SIGIR full papers and 215 TOIS papers published between 2006 and 2015. The original objective of the study was to identify IR effectiveness experiments that are seriously underpowered (i.e., the sample size is far too small so that the probability of missing a real difference is extremely high) or overpowered (i.e., the sample size is so large that a difference will be considered statistically significant even if the actual effect size is extremely small). However, it quickly became clear to us that many IR effectiveness papers either lack significance testing or fail to report p-values and/or test statistics, which prevents us from conducting power analysis. Hence we first report on how IR researchers (fail to) report on significance test results, what types of tests they use, and how the reporting practices may have changed over the last decade. From those papers that reported enough information for us to conduct power analysis, we identify extremely overpowered and underpowered experiments, as well as appropriate sample sizes for future experiments. The raw results of our systematic survey of 1,055 papers and our R scripts for power analysis are available online. Our hope is that this study will help improve the reporting practices and experimental designs of future IR effectiveness studies.},
address = {New York, NY, USA},
author = {Sakai, Tetsuya},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},
doi = {10.1145/2911451.2911492},
isbn = {9781450340694},
month = {jul},
pages = {5--14},
publisher = {ACM Press},
series = {SIGIR '16},
title = {{Statistical Significance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015}},
url = {https://dl.acm.org/doi/10.1145/2911451.2911492 https://www.slideshare.net/TetsuyaSakai/sigir2016},
year = {2016}
}
